{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "def load_stopwords():\n",
    "        \"\"\"\n",
    "        :return: 加载好的停用词列表\n",
    "        \"\"\"\n",
    "        with open('../hit_stopwords.txt', 'r', encoding='UTF-8') as f:\n",
    "            lines = f.readlines()\n",
    "            stopwords = []\n",
    "            for line in lines:\n",
    "                stopwords.append(line.replace('\\n', ''))\n",
    "        return stopwords"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T20:58:23.952068Z",
     "end_time": "2023-04-23T20:58:23.962070Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3279 - accuracy: 0.9263\n",
      "Test loss: 0.3279300034046173\n",
      "Test accuracy: 0.9263271689414978\n",
      "375/375 [==============================] - 1s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.93      0.93      5922\n",
      "           1       0.93      0.92      0.93      6077\n",
      "\n",
      "    accuracy                           0.93     11999\n",
      "   macro avg       0.93      0.93      0.93     11999\n",
      "weighted avg       0.93      0.93      0.93     11999\n",
      "\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3093 - accuracy: 0.9266\n",
      "Test loss: 0.3093382716178894\n",
      "Test accuracy: 0.9265772104263306\n",
      "375/375 [==============================] - 1s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.92      0.93      5922\n",
      "           1       0.93      0.93      0.93      6077\n",
      "\n",
      "    accuracy                           0.93     11999\n",
      "   macro avg       0.93      0.93      0.93     11999\n",
      "weighted avg       0.93      0.93      0.93     11999\n",
      "\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3347 - accuracy: 0.9182\n",
      "Test loss: 0.33470720052719116\n",
      "Test accuracy: 0.9181598424911499\n",
      "375/375 [==============================] - 1s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92      5999\n",
      "           1       0.92      0.92      0.92      6000\n",
      "\n",
      "    accuracy                           0.92     11999\n",
      "   macro avg       0.92      0.92      0.92     11999\n",
      "weighted avg       0.92      0.92      0.92     11999\n",
      "\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3288 - accuracy: 0.9240\n",
      "Test loss: 0.3288224935531616\n",
      "Test accuracy: 0.9239936470985413\n",
      "375/375 [==============================] - 1s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.91      0.92      6056\n",
      "           1       0.91      0.94      0.92      5943\n",
      "\n",
      "    accuracy                           0.92     11999\n",
      "   macro avg       0.92      0.92      0.92     11999\n",
      "weighted avg       0.92      0.92      0.92     11999\n",
      "\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3339 - accuracy: 0.9287\n",
      "Test loss: 0.3339095115661621\n",
      "Test accuracy: 0.9286607503890991\n",
      "375/375 [==============================] - 1s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.92      0.93      6008\n",
      "           1       0.92      0.94      0.93      5991\n",
      "\n",
      "    accuracy                           0.93     11999\n",
      "   macro avg       0.93      0.93      0.93     11999\n",
      "weighted avg       0.93      0.93      0.93     11999\n",
      "\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3298 - accuracy: 0.9248\n",
      "Test loss: 0.3298439681529999\n",
      "Test accuracy: 0.9248270392417908\n",
      "375/375 [==============================] - 1s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.92      0.92      5965\n",
      "           1       0.92      0.93      0.93      6034\n",
      "\n",
      "    accuracy                           0.92     11999\n",
      "   macro avg       0.92      0.92      0.92     11999\n",
      "weighted avg       0.92      0.92      0.92     11999\n",
      "\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3516 - accuracy: 0.9231\n",
      "Test loss: 0.3516134023666382\n",
      "Test accuracy: 0.9230769276618958\n",
      "375/375 [==============================] - 1s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.92      0.92      6035\n",
      "           1       0.92      0.93      0.92      5964\n",
      "\n",
      "    accuracy                           0.92     11999\n",
      "   macro avg       0.92      0.92      0.92     11999\n",
      "weighted avg       0.92      0.92      0.92     11999\n",
      "\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3117 - accuracy: 0.9262\n",
      "Test loss: 0.31172358989715576\n",
      "Test accuracy: 0.9261605143547058\n",
      "375/375 [==============================] - 1s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93      6116\n",
      "           1       0.93      0.92      0.92      5883\n",
      "\n",
      "    accuracy                           0.93     11999\n",
      "   macro avg       0.93      0.93      0.93     11999\n",
      "weighted avg       0.93      0.93      0.93     11999\n",
      "\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3407 - accuracy: 0.9248\n",
      "Test loss: 0.3406940996646881\n",
      "Test accuracy: 0.9248207807540894\n",
      "375/375 [==============================] - 1s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.92      0.92      6012\n",
      "           1       0.92      0.93      0.92      5986\n",
      "\n",
      "    accuracy                           0.92     11998\n",
      "   macro avg       0.92      0.92      0.92     11998\n",
      "weighted avg       0.92      0.92      0.92     11998\n",
      "\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3322 - accuracy: 0.9210\n",
      "Test loss: 0.33218875527381897\n",
      "Test accuracy: 0.9209868311882019\n",
      "375/375 [==============================] - 1s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92      5960\n",
      "           1       0.92      0.92      0.92      6038\n",
      "\n",
      "    accuracy                           0.92     11998\n",
      "   macro avg       0.92      0.92      0.92     11998\n",
      "weighted avg       0.92      0.92      0.92     11998\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from keras.utils import pad_sequences\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import numpy as np\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    # 读取我们本次实验的文本数据\n",
    "    df = pd.read_csv('../Emotional_text/weibo_sentiment.csv')\n",
    "\n",
    "    # 分词并去除停用词\n",
    "    stop_words = load_stopwords()\n",
    "    df['review'] = df['review'].apply(lambda x: re.sub(r'\\d+', '', x))\n",
    "    df['review'] = df['review'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "    df['review'] = df['review'].apply(lambda x: x.replace(\" \", \"\"))\n",
    "    df['review'] = df['review'].apply(lambda x: ' '.join(word for word in jieba.cut(x) if word not in stop_words))\n",
    "\n",
    "    # 使用Keras的Tokenizer来转化词语为词向量，这里我们选择出现频率前25000个词作为词袋\n",
    "    tokenizer = Tokenizer(num_words=25000, oov_token='<OOV>')\n",
    "    tokenizer.fit_on_texts(df['review'])\n",
    "    seq = tokenizer.texts_to_sequences(df['review'])\n",
    "\n",
    "    # 补齐每个样本序列，使其长度一样来方便后续的计算\n",
    "    max_len = 280\n",
    "    seq = pad_sequences(seq, maxlen=max_len, truncating='post')\n",
    "\n",
    "    # 划分数据集，这里我们使用十折交叉验证，循环十次训练\n",
    "    labels = df['label']\n",
    "    kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    losses = []\n",
    "    accs = []\n",
    "    for train_idx, test_idx in kfold.split(seq, labels):\n",
    "        # 划分训练集和测试集\n",
    "        train_seq, test_seq = seq[train_idx], seq[test_idx]\n",
    "        train_labels, test_labels = labels.iloc[train_idx], labels.iloc[test_idx]\n",
    "\n",
    "        # 建立一个神经网络模型，一层层添加神经元层\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(25000, 150, input_length=max_len))\n",
    "        model.add(Conv1D(128, 5, activation='relu'))\n",
    "        model.add(GlobalMaxPooling1D())\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "        # 编译刚刚建立的卷积神经网络模型\n",
    "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        # 设置 EarlyStopping 和 ReduceLROnPlateau 来避免过拟合\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=3)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.0001)\n",
    "\n",
    "        # 训练以及验证卷积神经网络模型的效果\n",
    "        history = model.fit(train_seq, tf.keras.utils.to_categorical(train_labels), batch_size=128, epochs=20, validation_data=(test_seq, tf.keras.utils.to_categorical(test_labels)), callbacks=[early_stop, reduce_lr], verbose=0)\n",
    "\n",
    "        # 评估本实验建立的卷积神经网络模型的训练结果的各项参数\n",
    "        loss, accuracy = model.evaluate(test_seq, tf.keras.utils.to_categorical(test_labels))\n",
    "        print('Test loss:', loss)\n",
    "        print('Test accuracy:', accuracy)\n",
    "        losses.append(loss)\n",
    "        accs.append(accuracy)\n",
    "        y_pred = np.argmax(model.predict(test_seq), axis=-1)\n",
    "        print(classification_report(test_labels, y_pred))\n",
    "\n",
    "    #     # 绘制训练集和测试集的损失和准确率变化曲线\n",
    "    #     import matplotlib.pyplot as plt\n",
    "    #     %matplotlib inline\n",
    "    #\n",
    "    #     plt.plot(history.history['accuracy'])\n",
    "    #     plt.plot(history.history['val_accuracy'])\n",
    "    #     plt.title('Model accuracy')\n",
    "    #     plt.ylabel('Accuracy')\n",
    "    #     plt.xlabel('Epoch')\n",
    "    #     plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    #     plt.show()\n",
    "    #\n",
    "    #     plt.plot(history.history['loss'])\n",
    "    #     plt.plot(history.history['val_loss'])\n",
    "    #     plt.title('Model loss')\n",
    "    #     plt.ylabel('Loss')\n",
    "    #     plt.xlabel('Epoch')\n",
    "    #     plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    #     plt.show()\n",
    "    #\n",
    "    # print('Mean loss:', np.mean(losses))\n",
    "    # print('Mean accuracy:', np.mean(accs))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T20:58:35.756030Z",
     "end_time": "2023-04-23T21:11:42.708780Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我真的好伤心要哭成泪人了有没有小伙伴来同情一下我啊我好难受\n",
      "['真的 好 伤心 哭 成 泪人 有没有 小伙伴 同情 一下 好 难受']\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# 准备数据\n",
    "new_text = \"我真的好伤心，要哭成泪人了，有没有小伙伴来同情一下我啊，我好难受\"\n",
    "\n",
    "new_text = re.sub(r'\\d+', '', new_text)\n",
    "\n",
    "new_text = re.sub(r'[^\\w\\s]', '', new_text)\n",
    "\n",
    "new_text = new_text.replace(\" \", \"\")\n",
    "\n",
    "# 分词并去除停用词\n",
    "new_text = [' '.join(word for word in jieba.cut(new_text) if word not in stop_words)]\n",
    "\n",
    "# 词向量化\n",
    "seq = tokenizer.texts_to_sequences(new_text)\n",
    "padded = pad_sequences(seq, maxlen=max_len, truncating='post')\n",
    "\n",
    "# 预测\n",
    "predictions = model.predict(padded)\n",
    "\n",
    "# 解析结果\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "print(predicted_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T21:13:30.229825Z",
     "end_time": "2023-04-23T21:13:30.297546Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
