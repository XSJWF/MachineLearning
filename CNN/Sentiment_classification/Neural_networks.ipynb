{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def load_stopwords():\n",
    "        \"\"\"\n",
    "        :return: 加载好的停用词列表\n",
    "        \"\"\"\n",
    "        with open('../hit_stopwords.txt', 'r', encoding='UTF-8') as f:\n",
    "            lines = f.readlines()\n",
    "            stopwords = []\n",
    "            for line in lines:\n",
    "                stopwords.append(line.replace('\\n', ''))\n",
    "        return stopwords"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-19T08:44:11.470022200Z",
     "start_time": "2023-05-19T08:44:11.454022600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " ...\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n",
      "1         1\n",
      "2         1\n",
      "3         1\n",
      "4         1\n",
      "5         1\n",
      "         ..\n",
      "119983    0\n",
      "119984    0\n",
      "119985    0\n",
      "119986    0\n",
      "119987    0\n",
      "Name: label, Length: 107989, dtype: int64\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3279 - accuracy: 0.9245\n",
      "Test loss: 0.3278740644454956\n",
      "Test accuracy: 0.9244937300682068\n",
      "375/375 [==============================] - 1s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.92      0.92      5922\n",
      "           1       0.92      0.93      0.93      6077\n",
      "\n",
      "    accuracy                           0.92     11999\n",
      "   macro avg       0.92      0.92      0.92     11999\n",
      "weighted avg       0.92      0.92      0.92     11999\n",
      "\n",
      "<class 'numpy.ndarray'>\n",
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " ...\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n",
      "0         1\n",
      "1         1\n",
      "2         1\n",
      "4         1\n",
      "5         1\n",
      "         ..\n",
      "119982    0\n",
      "119983    0\n",
      "119984    0\n",
      "119985    0\n",
      "119986    0\n",
      "Name: label, Length: 107989, dtype: int64\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 64\u001B[0m\n\u001B[0;32m     62\u001B[0m \u001B[38;5;28mprint\u001B[39m(train_labels)\n\u001B[0;32m     63\u001B[0m \u001B[38;5;66;03m# 训练以及验证卷积神经网络模型的效果\u001B[39;00m\n\u001B[1;32m---> 64\u001B[0m history \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_seq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkeras\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mutils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_categorical\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_labels\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m128\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtest_seq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkeras\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mutils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_categorical\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtest_labels\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mearly_stop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreduce_lr\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     66\u001B[0m \u001B[38;5;66;03m# 评估本实验建立的卷积神经网络模型的训练结果的各项参数\u001B[39;00m\n\u001B[0;32m     67\u001B[0m loss, accuracy \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mevaluate(test_seq, tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mto_categorical(test_labels))\n",
      "File \u001B[1;32mD:\\Anaconda3\\envs\\MachineLearning\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     63\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     64\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 65\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32mD:\\Anaconda3\\envs\\MachineLearning\\lib\\site-packages\\keras\\engine\\training.py:1555\u001B[0m, in \u001B[0;36mModel.fit\u001B[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m   1551\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m data_handler\u001B[38;5;241m.\u001B[39mcatch_stop_iteration():\n\u001B[0;32m   1552\u001B[0m     data_handler\u001B[38;5;241m.\u001B[39m_initial_step \u001B[38;5;241m=\u001B[39m data_handler\u001B[38;5;241m.\u001B[39m_initial_step \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[0;32m   1553\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_maybe_load_initial_step_from_ckpt()\n\u001B[0;32m   1554\u001B[0m     )\n\u001B[1;32m-> 1555\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m step \u001B[38;5;129;01min\u001B[39;00m data_handler\u001B[38;5;241m.\u001B[39msteps():\n\u001B[0;32m   1556\u001B[0m         \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mexperimental\u001B[38;5;241m.\u001B[39mTrace(\n\u001B[0;32m   1557\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   1558\u001B[0m             epoch_num\u001B[38;5;241m=\u001B[39mepoch,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1561\u001B[0m             _r\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[0;32m   1562\u001B[0m         ):\n\u001B[0;32m   1563\u001B[0m             callbacks\u001B[38;5;241m.\u001B[39mon_train_batch_begin(step)\n",
      "File \u001B[1;32mD:\\Anaconda3\\envs\\MachineLearning\\lib\\site-packages\\keras\\engine\\data_adapter.py:1374\u001B[0m, in \u001B[0;36mDataHandler.steps\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1372\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_insufficient_data:  \u001B[38;5;66;03m# Set by `catch_stop_iteration`.\u001B[39;00m\n\u001B[0;32m   1373\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m-> 1374\u001B[0m original_spe \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_steps_per_execution\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnumpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mitem()\n\u001B[0;32m   1375\u001B[0m can_run_full_execution \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m   1376\u001B[0m     original_spe \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   1377\u001B[0m     \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inferred_steps \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1378\u001B[0m     \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inferred_steps \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_current_step \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m original_spe\n\u001B[0;32m   1379\u001B[0m )\n\u001B[0;32m   1381\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m can_run_full_execution:\n",
      "File \u001B[1;32mD:\\Anaconda3\\envs\\MachineLearning\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:637\u001B[0m, in \u001B[0;36mBaseResourceVariable.numpy\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    635\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mnumpy\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    636\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m context\u001B[38;5;241m.\u001B[39mexecuting_eagerly():\n\u001B[1;32m--> 637\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_value\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnumpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    638\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m(\n\u001B[0;32m    639\u001B[0m       \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnumpy() is only available when eager execution is enabled.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mD:\\Anaconda3\\envs\\MachineLearning\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1157\u001B[0m, in \u001B[0;36m_EagerTensorBase.numpy\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1134\u001B[0m \u001B[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001B[39;00m\n\u001B[0;32m   1135\u001B[0m \n\u001B[0;32m   1136\u001B[0m \u001B[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1154\u001B[0m \u001B[38;5;124;03m    NumPy dtype.\u001B[39;00m\n\u001B[0;32m   1155\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1156\u001B[0m \u001B[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001B[39;00m\n\u001B[1;32m-> 1157\u001B[0m maybe_arr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_numpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[0;32m   1158\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m maybe_arr\u001B[38;5;241m.\u001B[39mcopy() \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(maybe_arr, np\u001B[38;5;241m.\u001B[39mndarray) \u001B[38;5;28;01melse\u001B[39;00m maybe_arr\n",
      "File \u001B[1;32mD:\\Anaconda3\\envs\\MachineLearning\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1123\u001B[0m, in \u001B[0;36m_EagerTensorBase._numpy\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1121\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_numpy\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m   1122\u001B[0m   \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1123\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_numpy_internal\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1124\u001B[0m   \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[0;32m   1125\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_status_to_exception(e) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from keras.utils import pad_sequences\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    # 读取我们本次实验的文本数据\n",
    "    df = pd.read_csv('../Emotional_text/weibo_sentiment.csv')\n",
    "\n",
    "    # 分词并去除停用词\n",
    "    stop_words = load_stopwords()\n",
    "    df['review'] = df['review'].apply(lambda x: re.sub(r'\\d+', '', x))\n",
    "    df['review'] = df['review'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "    df['review'] = df['review'].apply(lambda x: x.replace(\" \", \"\"))\n",
    "    df['review'] = df['review'].apply(lambda x: ' '.join(word for word in jieba.cut(x) if word not in stop_words))\n",
    "\n",
    "    # 使用Keras的Tokenizer来转化词语为词向量，这里我们选择出现频率前25000个词作为词袋\n",
    "    tokenizer = Tokenizer(num_words=25000, oov_token='<OOV>')\n",
    "    tokenizer.fit_on_texts(df['review'])\n",
    "    seq = tokenizer.texts_to_sequences(df['review'])\n",
    "\n",
    "    # 补齐每个样本序列，使其长度一样来方便后续的计算\n",
    "    max_len = 280\n",
    "    seq = pad_sequences(seq, maxlen=max_len, truncating='post')\n",
    "\n",
    "    # 划分数据集，这里我们使用十折交叉验证，循环十次训练\n",
    "    labels = df['label']\n",
    "    kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    losses = []\n",
    "    accs = []\n",
    "    for train_idx, test_idx in kfold.split(seq, labels):\n",
    "        # 划分训练集和测试集\n",
    "        train_seq, test_seq = seq[train_idx], seq[test_idx]\n",
    "        train_labels, test_labels = labels.iloc[train_idx], labels.iloc[test_idx]\n",
    "\n",
    "        # 建立一个神经网络模型，一层层添加神经元层\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(25000, 150, input_length=max_len))\n",
    "        model.add(Conv1D(128, 5, activation='relu'))\n",
    "        model.add(GlobalMaxPooling1D())\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "        # 编译刚刚建立的卷积神经网络模型\n",
    "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        # 设置 EarlyStopping 和 ReduceLROnPlateau 来避免过拟合\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=3)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.0001)\n",
    "\n",
    "        # 训练以及验证卷积神经网络模型的效果\n",
    "        history = model.fit(train_seq, tf.keras.utils.to_categorical(train_labels), batch_size=128, epochs=20, validation_data=(test_seq, tf.keras.utils.to_categorical(test_labels)), callbacks=[early_stop, reduce_lr], verbose=0)\n",
    "\n",
    "        # 评估本实验建立的卷积神经网络模型的训练结果的各项参数\n",
    "        loss, accuracy = model.evaluate(test_seq, tf.keras.utils.to_categorical(test_labels))\n",
    "        print('Test loss:', loss)\n",
    "        print('Test accuracy:', accuracy)\n",
    "        losses.append(loss)\n",
    "        accs.append(accuracy)\n",
    "        y_pred = np.argmax(model.predict(test_seq), axis=-1)\n",
    "        print(classification_report(test_labels, y_pred))\n",
    "\n",
    "        # 绘制训练集和测试集的损失和准确率变化曲线\n",
    "        import matplotlib.pyplot as plt\n",
    "        %matplotlib inline\n",
    "\n",
    "        plt.plot(history.history['accuracy'])\n",
    "        plt.plot(history.history['val_accuracy'])\n",
    "        plt.title('Model accuracy')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Test'], loc='upper left')\n",
    "        plt.show()\n",
    "\n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.plot(history.history['val_loss'])\n",
    "        plt.title('Model loss')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Test'], loc='upper left')\n",
    "        plt.show()\n",
    "\n",
    "    print('Mean loss:', np.mean(losses))\n",
    "    print('Mean accuracy:', np.mean(accs))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-19T09:01:41.256257700Z",
     "start_time": "2023-05-19T08:59:18.447658100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\DYL\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.735 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 16\u001B[0m\n\u001B[0;32m     13\u001B[0m new_text \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(word \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m jieba\u001B[38;5;241m.\u001B[39mcut(new_text) \u001B[38;5;28;01mif\u001B[39;00m word \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m stop_words)]\n\u001B[0;32m     15\u001B[0m \u001B[38;5;66;03m# 词向量化\u001B[39;00m\n\u001B[1;32m---> 16\u001B[0m seq \u001B[38;5;241m=\u001B[39m \u001B[43mtokenizer\u001B[49m\u001B[38;5;241m.\u001B[39mtexts_to_sequences(new_text)\n\u001B[0;32m     17\u001B[0m padded \u001B[38;5;241m=\u001B[39m pad_sequences(seq, maxlen\u001B[38;5;241m=\u001B[39mmax_len, truncating\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpost\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     19\u001B[0m \u001B[38;5;66;03m# 预测\u001B[39;00m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# 准备数据\n",
    "new_text = \"我真的好伤心，要哭成泪人了，有没有小伙伴来同情一下我啊，我好难受\"\n",
    "\n",
    "new_text = re.sub(r'\\d+', '', new_text)\n",
    "\n",
    "new_text = re.sub(r'[^\\w\\s]', '', new_text)\n",
    "\n",
    "new_text = new_text.replace(\" \", \"\")\n",
    "\n",
    "# 分词并去除停用词\n",
    "new_text = [' '.join(word for word in jieba.cut(new_text) if word not in stop_words)]\n",
    "\n",
    "# 词向量化\n",
    "seq = tokenizer.texts_to_sequences(new_text)\n",
    "padded = pad_sequences(seq, maxlen=max_len, truncating='post')\n",
    "\n",
    "# 预测\n",
    "predictions = model.predict(padded)\n",
    "\n",
    "# 解析结果\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "print(predicted_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T10:56:02.453148200Z",
     "start_time": "2023-05-13T10:56:01.694428800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
